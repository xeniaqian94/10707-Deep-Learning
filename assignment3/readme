Here is a list of commands to run the results for Question 3.

3.1 & 3.2 
	python train.py [no arguments needed, otherwise, please refer the code for options]

The model takes around 1.5 hrs to train 100 epochs under a normal PC. Even with Ctrl(cmd)+C intervention, intermediate best models are saved under dump/ directory. 

The vocabulary and its statistics are pickled as vocab_[arugments] under dump/ directory. 

3.3
	python train.py -activation tanh

3.4
	python lang_generation.py [model_variant_name]
	e.g. python lang_generation.py -lr_1.0_-max_epoch_20

	python similar_words.py -lr_1.0_-max_epoch_200
3.5
    python visualize_embedding.py [model_variant_name]
    python visualize_embedding.py -lr_1.0_-max_epoch_200


3.6 The following script requires PyTorch to be installed. 
	python pytorch_train_RNN.py [embed_size]
	Please then check log_pytorch_rnn_[variant] for loss and perplexity across epoch time 

To visualize RNN embeddings, 
	python visualize_embedding.py -lr_1.0_-max_epoch_200
To plot based on logs under dump/, invoke python plot.py dump/log [variants]*

For any question, please contact Xin Qian at email xinq@andrew.cmu.edu


